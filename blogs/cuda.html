<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CUDA learning</title>
    <style>
        :root {
            --primary-color: #3498db;
            --secondary-color: #2c3e50;
            --background-color: #f9f9f9;
            --text-color: #333;
        }
        body {
            font-family: 'Arial', sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
        }
        header {
            background-color: var(--secondary-color);
            color: white;
            padding: 1rem;
            text-align: center;
        }
        main {
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem;
        }
        h1 {
            color: var(--primary-color);
        }
        .meta {
            font-style: italic;
            color: #666;
            margin-bottom: 1rem;
        }
        .content {
            background-color: white;
            border: 1px solid #ddd;
            padding: 2rem;
        }
        .back-link {
            display: inline-block;
            margin-top: 1rem;
            color: var(--primary-color);
            text-decoration: none;
        }
        .back-link:hover {
            text-decoration: underline;
        }
        .code-block-small {
            font-size: 0.6em;
        }
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-c.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-cpp.min.js"></script>
</head>
<body>

    <main>
        <h1>Learning CUDA: Taming the GPU</h1>
        <div class="meta">August 2024</div>
        <div class="content">
            <h2>Introduction</h2>

            <p> I first heard about CUDA last year when I was trying to create pretty pictures called fractals.
                Skipping all the math mumbo jumbo, fractals are basically geometric patterns are self similar.
                You might have heard of the famous Mandelbrot set, that's a fractal! This is what it looks like:
            </p>

            <img src="../resources/mandelbrot.jpg" alt="Mandelbrot set">

            <p> 
                Anyways, rendering this uses a fun little algorithm called the escape time algorithm.
                Basically you give every pixel a starting value, and then you repeatedly apply a mathematical function to it
                over and over again. If the absolute value of the result is greater than 2, we say that the sequence has "escaped"
                and the starting pixel is colored based on how many iterations it took to escape. If the sequence never escapes,
                the pixel is colored based on how many iterations it took to converge.
            </p>

            <p>
                This is all the math you need to know for now, but if you're interested in the details you can read more 
                <a href="https://en.wikipedia.org/wiki/Mandelbrot_set">here</a>.
            </p>

            <h2>The struggle</h2>
            <p>
                Pretty basic algorithm right? Well, it turns out that rendering fractals is a compute-heavy task.
                The good thing is that each pixel is independent of the others, so it's a perfect task for a GPU.
                each computation is simple, and GPUs are designed to perform many of them at the same time.
            </p>

            <p> 
                The bad thing is that I dont have a GPU, I have a potato from 2019. So the first task was to get my hands on a GPU.
                Luckily, I found this website called <a href="https://vast.ai/">Vast AI</a> that lets you rent consumer GPUs for really cheap.
            </p>

            <h2>Okay but what is CUDA?</h2>

            <p>CUDA is a parallel computing platform and programming model created by NVIDIA. It allows developers to use the GPU's
                parallel computing power to perform tasks that were previously the domain of CPUs.
            </p>
            
            <figure>
                <img src="../resources/NVIDIA-CUDA-memory-model.png" alt="CUDA memory model" width="100%" height="100%">
                <figcaption>CUDA memory model</figcaption>
            </figure>

            <p>
                CUDA divides the work among many small processors called threads. These threads are grouped into smaller units called blocks,
                and these blocks are further grouped into even larger units called grid. Threads within the same block can cooperate with each other,
                but threads in different blocks cannot. This is called <b>cooperative multithreading</b>.
            </p>

            <p>My go to resource for learning CUDA was this book called "Programming Massively Parallel Processors"</p>

            <h2>Baby's first CUDA program</h2>

            <p>Starting with the basics, here's a simple CUDA program that adds two arrays:</p>

            <pre class="code-block-small"><code class="language-cpp">__global__ void vectorAddKernel(const float *a, const float *b, float *c, int n) {
    int idx = blockDim.x * blockIdx.x + threadIdx.x;
    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}</code></pre>

            <p>Note that this is only the kernel, we will also need some cudaMalloc and cudaMemcpy calls to actually use this kernel.</p>
            <p>The full code can be found <a href="https://github.com/AadityaSuri/cuda_learning/blob/master/vec_add/vec_add.cu">here</a>.</p>

            <h3>Explanation</h3>

            <p>
                The __global__ keyword is used to declare a function that will be executed on the GPU.
                The function takes in four arguments: two input arrays a and b, an output array c, and the size of the arrays n.
                The function calculates the index of the thread in the grid using the blockDim and blockIdx variables.
                The threadIdx variable is used to get the index of the thread within the block.
            </p>

            <p>
                The if statement is used to ensure that the thread does not access out of bounds memory.
                The thread then adds the corresponding elements of a and b and stores the result in c.
            </p>

            <h2>Adding some dimensions</h2>

            <p>CUDA allows us to add multiple dimensions to our kernels, here's an example of a 2D kernel that adds two matrices:</p>

            <pre class="code-block-small"><code class="language-cpp">__global__ void double_matrix_x_y(const float *a, float *res, int rows, int cols) {
    int tid_x = blockDim.x * blockIdx.x + threadIdx.x;
    int tid_y = blockDim.y * blockIdx.y + threadIdx.y;

    int idx;
    if (tid_x < cols && tid_y < rows) {
        idx = tid_y * cols + tid_x;

        res[idx] = a[idx] * 2;
    }
}</code></pre>

            <h3>Explanation</h3>

            <p>
            Let's break down this 2D CUDA kernel in a more approachable way:

            <ol>
                <li>Thread Identification:
                   <p>`tid_x` and `tid_y` calculate the unique position of each thread in the 2D grid. Think of it as assigning coordinates to each worker in a large warehouse.</p>
                </li>

                <li>Boundary Check:
                   <p>The `if` statement ensures that each thread only operates within the matrix dimensions. It's like making sure each worker stays within their designated area.</p>
                </li>

                <li>Flattening 2D to 1D:
                   <p>`idx = tid_y * cols + tid_x` converts the 2D position to a 1D index. This is similar to how we might number shelves in a warehouse - row by row.</p>
                </li>

                <li>The Operation:
                   <p>`res[idx] = a[idx] * 2` is where each thread performs its task. In this case, it's doubling the value at its assigned position.</p>
                </li>
            </ol>

            This kernel efficiently parallelizes the process of doubling each element in a 2D matrix, with each thread handling one element simultaneously.
            </p>

            <p>The full code for this can be found <a href="https://github.com/AadityaSuri/cuda_learning/blob/master/2d_func/2d_array_func.cu">here</a>.</p>

            <h2>Back to fractals</h2>

            <p>
                Now that we have some experience under our belt, let's try to render a Mandelbrot set using CUDA!
            </p>

            <p>
                Basically we just need to modify our 2D kernel to perform the escape time algorithm.
                We also need a way to store the result, and to do this we can use cudaMalloc to allocate memory for the image.
                We also need to make sure that our kernel is launched with the correct number of threads and blocks.
                This is where the concepts of threads, blocks and grids come in.
            </p>

            <p>This is what the CUDA kernel looks like:</p>

            <pre class="code-block-small"><code class="language-cpp">__global__ void mandelbrotKernel(unsigned char* image, int height, int width, 
                                 double x_min, double x_max, double y_min, double y_max, 
                                 int max_iteration) {
    int i = blockIdx.y * blockDim.y + threadIdx.y;
    int j = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (i < height && j < width) {
        double x = 0;
        double y = 0;
        int iteration = 0;
        double x_scaled = x_min + j * (x_max - x_min) / (width - 1.0);
        double y_scaled = y_min + i * (y_max - y_min) / (height - 1.0);
        
        while (x*x + y*y <= 4 && iteration < max_iteration) {
            double xtemp = x*x - y*y + x_scaled;
            y = 2*x*y + y_scaled;
            x = xtemp;
            iteration++;
        }
        
        int r = 0, g = 0, b = 0;
        if (iteration < max_iteration) {
            int colorIndex = iteration % 16;
            r = d_colorMap[colorIndex][0];
            g = d_colorMap[colorIndex][1];
            b = d_colorMap[colorIndex][2];
        }
        
        int index = (i * width + j) * BYTES_PER_PIXEL;
        image[index + 2] = (unsigned char) r;  // Red
        image[index + 1] = (unsigned char) g;  // Green
        image[index] = (unsigned char) b;      // Blue
    }
}</code></pre>

            <p>Here are some results!</p>

            <img src="../resources/combined_mdbrot.jpg" alt="Mandelbrot set" width="100%" height="100%">

            <h3>Speedup</h3>

            <p>using CUDA also achieved significant speedup in rendering these images, take the following times for example:</p>

            <img src="../resources/speedup.png">

            <p><strong>almost 40x speedup!</strong></p>

            <p>The code for this can be found <a href="https://github.com/AadityaSuri/Mandelbrot_CUDA/">here</a>. main.cu contains all the cuda code and some other stuff needed to generate these images</p>

            
            
            <!-- Add more content as needed -->
            <h2>Matrix multiplication</h2>

            <p>Other than just making pictures, one of main use cases of CUDA is to perform really fast matrix multiplication.</p>

            <p>There are a few ways to write efficient kernels for matrix multiplication, I explore some of them in this 
                <a href="https://github.com/AadityaSuri/cuda_learning/blob/master/matmul/matmul.cu">file</a>.
            </p>    

            <p>A very detailed guide to matrix multiplication kernels in CUDA by Simon Boehm can be found <a href="https://siboehm.com/articles/22/CUDA-MMM">here</a>.</p>
           
            <h2>Next steps</h2>

            <p>There is still a lot more to learn! Some of the things I plan to explore in the near future are:

                <ul>
                    <li>Implementing a basic Flash Attention kernel</li>
                    <li>Writing a CNN from scratch using CUDA</li> 
                </ul>
            </p>
        </div>
    </main>
</body>
</html>
